{
  "id": "canva-machine learning engineer-senior-optimize-machine-learning-model-serving-latency",
  "title": "Optimize Machine Learning Model Serving Latency",
  "statement": "You are responsible for deploying and serving a machine learning model that predicts the likelihood of a user clicking on a particular Canva template recommendation.  The model is currently deployed as a REST API endpoint and receives thousands of requests per second.  However, the serving latency is higher than desired (e.g., consistently above 100ms).  \n\nDescribe the steps you would take to identify the bottlenecks contributing to the high latency and implement optimizations to reduce it.  Specifically, consider the following:\n\n*   **Profiling and Monitoring:** How would you profile and monitor the performance of the serving system to identify bottlenecks (e.g., using tools like Prometheus, Grafana, tracing)?\n*   **Hardware Optimization:** Are there any hardware-level optimizations you could consider (e.g., GPU acceleration, CPU optimization, memory allocation)?\n*   **Model Optimization:** Could the model itself be optimized for faster inference (e.g., model quantization, pruning, knowledge distillation)?\n*   **Serving Infrastructure:** Are there any optimizations you could make to the serving infrastructure (e.g., caching, load balancing, request batching, asynchronous processing)?  Consider that you may be using Kubernetes for deployment.\n*   **Code Optimization:** Are there any code-level optimizations you could perform in the model serving code (e.g., efficient data structures, avoiding unnecessary computations)?\n\nOutline a systematic approach to latency optimization, including the tools and techniques you would use at each stage. Provide justifications for your choices. Also, explain how you would measure the impact of each optimization before and after implementation.  Assume that high accuracy is critical.\n",
  "languages": [
    "python",
    "java",
    "cpp"
  ],
  "stub": {
    "cpp": "class Solution {\npublic:\n    void optimizeServingLatency() {\n        // Your code here: Provide a high-level plan in comments.\n        // 1. Profiling and Monitoring (Tools: Prometheus, Grafana, tracing)\n        // 2. Hardware Optimization (GPU, CPU)\n        // 3. Model Optimization (Quantization, Pruning)\n        // 4. Serving Infrastructure (Caching, Load Balancing, Batching, Async)\n        // 5. Code Optimization (Data Structures, Algorithm Efficiency)\n    }\n};",
    "java": "class Solution {\n    public void optimizeServingLatency() {\n        // Your code here: Provide a high-level plan in comments.\n        // 1. Profiling and Monitoring (Tools: Prometheus, Grafana, tracing)\n        // 2. Hardware Optimization (GPU, CPU)\n        // 3. Model Optimization (Quantization, Pruning)\n        // 4. Serving Infrastructure (Caching, Load Balancing, Batching, Async)\n        // 5. Code Optimization (Data Structures, Algorithm Efficiency)\n    }\n}",
    "python": "def optimize_serving_latency():\n    # Your code here: Provide a high-level plan in comments.\n    # 1. Profiling and Monitoring (Tools: Prometheus, Grafana, tracing)\n    # 2. Hardware Optimization (GPU, CPU)\n    # 3. Model Optimization (Quantization, Pruning)\n    # 4. Serving Infrastructure (Caching, Load Balancing, Batching, Async)\n    # 5. Code Optimization (Data Structures, Algorithm Efficiency)\n    pass"
  }
}